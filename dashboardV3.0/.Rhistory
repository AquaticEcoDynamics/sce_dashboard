#encoding = "UTF-8",
# show_col_types = FALSE,
# lazy = FALSE,
# progress = FALSE,
object = filename,
bucket = bucket,
filename = basename(filename),
opts = list(
base_url = "projects.pawsey.org.au",
region = "",
key = "2f1a9d81bdf24a178b2bd18d530e959b",
secret = "e062073c1faf488cb4209ba8de2eb483"))
return(fetchedData)
}
#                                       object = filename,
#                                       bucket = bucket,
#                                       filename = basename(filename),
#                                       opts = list(
#                                         base_url = "projects.pawsey.org.au",
#                                         region = ""))
#
#   return(fetchedData)
# }
#Rawdata <- read.csv(file = 'www/DBCA_data_export/DBCA_data_export_2023-07-19_1615.csv', check.names = FALSE)
Rawdata <- awss3Connect(filename = 'arms/wiski.csv')
dat<-Rawdata%>%
select(1,2,3,4,5,6,7,8,11,12,13,14,21,24,27,30,33,36,39,47,48,50,51,53,54,56,57,59,60,63,66,68,69,71,72,77,78,86,87)
#1.3 rename columns to something easier to code with
# dat <-dat %>% rename(Time = Collect.Time, Sample.Date = Collect.Date, Month = Collect.Month, Year = Collect.Year,
#                      Project = Program.Code, Site = Program.Site.Ref, Sal.ppt =Salinity..ppt., Temp=Temperature..deg.C.,
#                      ODO.mgl=O2..DO.conc...mg.L.,ODO.sat=O2..DO..sat....., pH=pH..no.units.,Turbidity.NTU=Turbidity..NTU.,
#                      TON.mgl=N..sum.sol.ox...NOx.N.TON...ug.L.,NH3.mgl=NH3.N.NH4.N..sol...ug.L.,DON.mgl=N..sum.sol.org...DON...ug.L.,
#                      TKN.mgl=N..tot.kjel...TKN...ug.L.,NO2.mgl=NO2.N..sol...ug.L.,NO3.mgl=NO3.N..sol...ug.L.,TP.mgl=P..tot...TP.pTP...ug.L.,
#                      FRP.mgl=PO4.P..sol.react...SRP.FRP...ug.L.,DOC.mgl=C..sol.org...DOC.DOC.as.NPOC...ug.L.,TSS.mgl=Suspended.Solids..Total...TSS...mg.L.,TN.mgl=N..tot...TN.pTN...ug.L.,
#                      SPCcondus.cm = Cond...25.deg.C..uS.cm., COC = Primary.CoC.Number)
dat <-dat %>% rename(#Site.Ref = `ï»¿Site Ref`,
Site.Ref = `Site Ref`,
Time = `Collect Time`,
Sample.Date = `Collect Date`,
Month = `Collect Month`,
Year = `Collect Year`,
Project = `Program Code`,
Site = `Program Site Ref`,
Sal.ppt = `Salinity (ppt)`,
Temp= `Temperature (deg C)`,
ODO.mgl=`O2-{DO conc} (mg/L)`,
ODO.sat=`O2-{DO %sat} (%)`,
pH = `pH (no units)`,
Turbidity.NTU= `Turbidity (NTU)`,
TON.mgl= `N (sum sol ox) {NOx-N TON} (ug/L)`,
NH3.mgl= `NH3-N/NH4-N (sol) (ug/L)`,
DON.mgl= `N (sum sol org) {DON} (ug/L)`,
TKN.mgl= `N (tot kjel) {TKN} (ug/L)`,
NO2.mgl= `NO2-N (sol) (ug/L)`,
NO3.mgl= `NO3-N (sol) (ug/L)`,
TP.mgl = `P (tot) {TP pTP} (ug/L)`,
FRP.mgl= `PO4-P (sol react) {SRP FRP} (ug/L)`,
DOC.mgl= `C (sol org) {DOC DOC as NPOC} (ug/L)`,
TSS.mgl= `Suspended Solids (Total) {TSS} (mg/L)`,
TN.mgl = `N (tot) {TN pTN} (ug/L)`,
SPCcondus.cm = `Cond @ 25 deg C (uS/cm)`,
COC = `Primary CoC Number`)
#1.4 Convert analyte units from ug/L to mg/L.
dat$TN.mgl<-dat$TN.mgl*0.001
dat$TON.mgl<-dat$TON.mgl*0.001
dat$NH3.mgl<-dat$NH3.mgl*0.001
dat$DON.mgl<-dat$DON.mgl*0.001
dat$TKN.mgl<-dat$TKN.mgl*0.001
dat$TP.mgl<-dat$TP.mgl*0.001
dat$FRP.mgl<-dat$FRP.mgl*0.001
dat$DOC.mgl<-dat$DOC.mgl*0.001
View(dat)
dat<-dat%>%
unite(TN.mgl, c("TN Sign","TN.mgl"))%>%
unite(TON.mgl, c("NOx-N Sign","TON.mgl"))%>%
unite(NH3.mgl, c("NH3/4-N Sign","NH3.mgl"))%>%
unite(DON.mgl, c("DON Sign","DON.mgl"))%>%
unite(TKN.mgl, c("TKN Sign","TKN.mgl"))%>%
unite(TP.mgl, c("TP Sign","TP.mgl"))%>%
unite(FRP.mgl, c("FRP Sign","FRP.mgl"))%>%
unite(TSS.mgl, c("TSS Sign","TSS.mgl"))%>%
unite(DOC.mgl, c("DOC Sign","DOC.mgl"))
#1.6 remove "_" between '<' symbol and analyte value
dat$TN.mgl<-gsub("_","",as.character(dat$TN.mgl))
dat$TON.mgl<-gsub("_","",as.character(dat$TON.mgl))
dat$NH3.mgl<-gsub("_","",as.character(dat$NH3.mgl))
dat$DON.mgl<-gsub("_","",as.character(dat$DON.mgl))
dat$TKN.mgl<-gsub("_","",as.character(dat$TKN.mgl))
dat$TP.mgl<-gsub("_","",as.character(dat$TP.mgl))
dat$FRP.mgl<-gsub("_","",as.character(dat$FRP.mgl))
dat$TSS.mgl<-gsub("_","",as.character(dat$TSS.mgl))
dat$DOC.mgl<-gsub("_","",as.character(dat$DOC.mgl))
#1.7 halving less than values (standard procedure) so they can be converted into numerics
dat$TN.mgl[dat$TN.mgl == "<0.025"] <- "0.0125"
dat$TSS.mgl[dat$TSS.mgl == "<1"] <- "0.5"
dat$FRP.mgl[dat$FRP.mgl == "<0.005"] <- "0.0025"
dat$TON.mgl[dat$TON.mgl == "<0.01"] <- "0.005"
dat$NH3.mgl[dat$NH3.mgl == "<0.01"] <- "0.005"
dat$TP.mgl[dat$TP.mgl == "<0.005"] <- "0.0025"
dat$TN.mgl[dat$TN.mgl == "<0.025"] <- "0.5"
dat$DON.mgl[dat$DON.mgl == "<0.025"] <- "0.0125"
dat$TKN.mgl[dat$TKN.mgl == "<0.025"] <- "0.0125"
dat$DOC.mgl[dat$DOC.mgl == "<1"] <- "0.5"
#1.8 remove "NA"s from analyte columns
dat[dat=="NA"]<-""
#1.9 removes records with no sample numbers (essentially sites that weren't sampled due to them not flowing etc.) from 2100 to 1841
#dat<-dat[!dat$Sample.Number=="",];
dat<-dat[!dat$`Sample Number`=="",];
#head(dat)
sapply(dat, class)
View(dat)
renv::restore(confirm = FALSE)
library(dplyr)
library(readr)
library(lubridate)
library(hms) # might need to install this one
LIMS_parser <- function(data_location, skip = 8){
# find names for full paths to a weeks worth of data sets
data_todo <- list.files(path = data_location, pattern = ".csv",
full.names = TRUE)
# set up site df's in correct upriver order
s_names <- c("BLA", "ARM", "HEA", "NAR", "NIL", "STJ", "MAY", "RON", "KIN",
"SUC", "WMP", "MSB")
c_names <- c("SCB2", "SAL", "SHELL", "RIV", "CASMID", "KEN", "BAC", "NIC", "ELL")
df_swan <- dplyr::data_frame(site = s_names)
df_cann <- dplyr::data_frame(site = c_names)
# create summary of all sites for week
df_summary <- dplyr::data_frame()
for(i in seq_along(data_todo)){
# get column headers
col_names <- names(read_csv(data_todo[i], n_max = 0))
# read in data, add back headers and skip metadata type entries
df <- readr::read_csv(data_todo[i], col_names = col_names, skip = skip)
# standard names - easier
names(df) <- tolower(col_names)
# munge and mutate
df_grab <- df %>%
dplyr::filter(`collection method` == "ID") %>%
dplyr::select(`site reference`, `date collected`,
`time`, contains("chlorophyll a" )) #can have more than 1!
df_out <- df_grab[colSums(!is.na(df_grab)) > 0] %>%
dplyr::rename(site = `site reference`,
date = `date collected`,
chla_mg = starts_with("chlorophyll")) %>%
dplyr::mutate(chla_ug = as.numeric(chla_mg) * 1000,
date = lubridate::dmy(date),
time = hms::as_hms(time),
category = case_when(
chla_ug <= 3.999 ~ "LOW",
chla_ug <= 10 ~ "MEDIUM",
chla_ug > 10 ~ "HIGH",
TRUE ~ "LOW"))
# add to summary data
df_summary <- dplyr::bind_rows(df_summary, df_out)
}
# match to separate river df's to maintain correct order, join back together
a <- dplyr::left_join(df_swan, df_summary, by = "site") %>%
dplyr::mutate(river = "swan")
b <- dplyr::left_join(df_cann, df_summary, by = "site") %>%
dplyr::mutate(river = "cann")
c <- dplyr::bind_rows(a, b)
# create appropriate name and save results
out_name <- paste0("./Output", "/",
dplyr::pull(c[1, 2]), "_interim_pigment_results.csv")
readr::write_csv(c, out_name)
}
data_location <- 'C:/Users/00101765/AED Dropbox/Sherry Zhai/SCEVO/data/Microalgae/23S0552.csv'
data_location <- 'C:/Users/00101765/AED Dropbox/Sherry Zhai/SCEVO/data/Microalgae/'
# find names for full paths to a weeks worth of data sets
data_todo <- list.files(path = data_location, pattern = ".csv",
full.names = TRUE)
# set up site df's in correct upriver order
s_names <- c("BLA", "ARM", "HEA", "NAR", "NIL", "STJ", "MAY", "RON", "KIN",
"SUC", "WMP", "MSB")
c_names <- c("SCB2", "SAL", "SHELL", "RIV", "CASMID", "KEN", "BAC", "NIC", "ELL")
df_swan <- dplyr::data_frame(site = s_names)
df_cann <- dplyr::data_frame(site = c_names)
# create summary of all sites for week
df_summary <- dplyr::data_frame()
i=1
# get column headers
col_names <- names(read_csv(data_todo[i], n_max = 0))
# read in data, add back headers and skip metadata type entries
df <- readr::read_csv(data_todo[i], col_names = col_names, skip = skip)
skip=8
# read in data, add back headers and skip metadata type entries
df <- readr::read_csv(data_todo[i], col_names = col_names, skip = skip)
View(df)
# standard names - easier
names(df) <- tolower(col_names)
# munge and mutate
df_grab <- df %>%
dplyr::filter(`collection method` == "ID") %>%
dplyr::select(`site reference`, `date collected`,
`time`, contains("chlorophyll a" )) #can have more than 1!
View(df_grab)
df_out <- df_grab[colSums(!is.na(df_grab)) > 0] %>%
dplyr::rename(site = `site reference`,
date = `date collected`,
chla_mg = starts_with("chlorophyll")) %>%
dplyr::mutate(chla_ug = as.numeric(chla_mg) * 1000,
date = lubridate::dmy(date),
time = hms::as_hms(time),
category = case_when(
chla_ug <= 3.999 ~ "LOW",
chla_ug <= 10 ~ "MEDIUM",
chla_ug > 10 ~ "HIGH",
TRUE ~ "LOW"))
View(df_out)
# add to summary data
df_summary <- dplyr::bind_rows(df_summary, df_out)
View(df_summary)
# match to separate river df's to maintain correct order, join back together
a <- dplyr::left_join(df_swan, df_summary, by = "site") %>%
dplyr::mutate(river = "swan")
View(a)
b <- dplyr::left_join(df_cann, df_summary, by = "site") %>%
dplyr::mutate(river = "cann")
c <- dplyr::bind_rows(a, b)
View(c)
#' awss3Connect
#'
#' @description Establishes connection to the S3 bucket and fetches data
#'
#' @return The return value, if any, from executing the function.
#' @import aws.s3 readr
#' @noRd
library('aws.s3')
# To enforce HTTPS, should be set to TRUE
Sys.setenv('USE_HTTPS' = TRUE)
# Set details for bucket origin
Sys.setenv(
'AWS_DEFAULT_REGION' = '',
'AWS_S3_ENDPOINT' = 'projects.pawsey.org.au',
'AWS_ACCESS_KEY_ID' = '2f1a9d81bdf24a178b2bd18d530e959b',
'AWS_SECRET_ACCESS_KEY' = 'e062073c1faf488cb4209ba8de2eb483'
)
awss3Connect <- function(filename){
# Now set bucket contents as objects
bucket <- 'scevo-data'
#filename = 'data-warehouse/dbca/wiski/DBCA_data_export_2023-07-19_1615.csv'
# fetchedData <- aws.s3::s3read_using(FUN = utils::read.csv,
#                                     check.names = FALSE,
#                                     encoding = "UTF-8",
#                                # show_col_types = FALSE,
#                                # lazy = FALSE,
#                                # progress = FALSE,
#                                object = filename,
#                                bucket = bucket,
#                                filename = basename(filename),
#                                opts = list(
#                                  base_url = "projects.pawsey.org.au",
#                                  region = ""))
fetchedData <- aws.s3::s3read_using(FUN = utils::read.csv,
check.names = FALSE,
#encoding = "UTF-8",
# show_col_types = FALSE,
# lazy = FALSE,
# progress = FALSE,
object = filename,
bucket = bucket,
filename = basename(filename),
opts = list(
base_url = "projects.pawsey.org.au",
region = "",
key = "2f1a9d81bdf24a178b2bd18d530e959b",
secret = "e062073c1faf488cb4209ba8de2eb483"))
return(fetchedData)
}
#' microalgaeactivityUI
#'
#' @description Generates the profile plots UI elements e.g. date inputs, region. Requires
#' @param namespace to individualise elements.
#'
#' @return The return value, if any, from executing the function.
#'
#' @noRd
#'
rawwiski <- awss3Connect(filename = 'arms/wiski.csv')
# set up site df's in correct upriver order
algaesites <- c("BLA", "ARM", "HEA", "NAR", "NIL", "STJ", "MAY", "RON", "KIN",
"SUC", "WMP", "MSB",  #swan
"SCB2", "SAL", "SHELL", "RIV", "CASMID", "KEN", "BAC", "NIC", "ELL") #canning
algaedf <- rawwiski %>%
dplyr::filter(`Program Site Ref` %in% algaesites &
`Collection Method` %in% 'Integrated over depth'&
`Data Category` %in% 'Laboratory results')%>%
dplyr::select(`Program Site Ref`, `Collect Date`,`Collect Month`,`Collect Year`, `Chlorophyll a (by vol) (mg/L)`)
View(algaedf)
algaedf$chla_ugL <- algaedf$`Chlorophyll a (by vol) (mg/L)`*1000
shiny::runApp()
Estuary_data<- read.csv("www/SCESTUARY - Copy.csv")
algae_data <-  Estuary_data %>%
dplyr::filter(Project.Site.Reference %in% algaesites)
View(algae_data)
algaedf$`Collect Date` <- as.Date(algaedf$`Collect Date`,format="%d/%m/%Y")
View(Estuary_data)
Estuary_data <- Estuary_data %>%
rename('Program Site Ref' = Project.Site.Reference)
algaedf_join <- inner_join(algaedf,Estuary_data)
algaedf_join <- inner_join(algaedf,Estuary_data,by = 'Program Site Ref')
View(algaedf_join)
runApp()
runApp()
runApp()
algaedf_join$activity <-
if(chla_ugL < 4) {
"LOW"
} else if(chla_ugL <= 10) {  #there's no option 'yellow'....
"MEDIUM"
} else if(chla_ugL > 10) {
"HIGH"
} else {
"UNKNOWN"
}
algaedf_join$activity <-
if(algaedf_join$chla_ugL < 4) {
"LOW"
} else if(algaedf_join$chla_ugL <= 10) {  #there's no option 'yellow'....
"MEDIUM"
} else if(algaedf_join$chla_ugL > 10) {
"HIGH"
} else {
"UNKNOWN"
}
selectedData <- algaedf_join %>%
dplyr::mutate(
category = case_when(
chla_ug < 4 ~ "LOW",
chla_ug <= 10 ~ "MEDIUM",
chla_ug > 10 ~ "HIGH",
TRUE ~ "LOW"))
selectedData <- algaedf_join %>%
dplyr::mutate(
category = case_when(
chla_ugL < 4 ~ "LOW",
chla_ugL <= 10 ~ "MEDIUM",
chla_ugL > 10 ~ "HIGH",
TRUE ~ "LOW"))
View(selectedData)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
gc()
runApp()
algaekey <- data.frame(
Level = c("Low","Medium","High","Alert"),
Description = c("Low levels of microalgae with no likely visible impact",
"Possibility of discolouration and/or scum formation",
"High probability of discolouration and/or scum formation",
"Presence of species potentially harmful to aquatic life, OR human health requiring public advice"),
`Chlorophyll-a (ug/L)` = c("< 4", "4 - 10", "> 10", "N/A")
)
View(algaekey)
runApp()
runApp()
gc()
runApp()
shiny::runApp()
runApp()
renv::status
renv::status()
?renv::status()
runApp()
runApp()
runApp()
#s1 <- read.csv("C:\\Users\\AminaSaeed\\Desktop\\old_one\\inst\\extdata\\sensorslist.csv") ### pcik DO for the list
#s1 <- read.csv("inst/extdata/sensorInfo.csv")
s1 <- read.csv("www/sensorslist.csv")
#### remove unnecessary columns for catchment
S2<-s1[!grepl("Site Ref", s1$s_graph_value),]
S3<-S2[!grepl("Sample Type", S2$s_graph_value),]
S4<-S3[!grepl("Program Site Ref", S3$s_graph_value),]
S5<-S4[!grepl("Program Code", S4$s_graph_value),]
S6<-S5[!grepl("Collection Method", S5$s_graph_value),]
S7<-S6[!grepl("Depth measuring point", S6$s_graph_value),]
S8<-S7[!grepl("Depth _s (m)", S7$s_graph_value),]
S9<-S8[!grepl("Depth (m)", S8$s_graph_value),]
S10<-S9[!grepl("Depth To (m)", S9$s_graph_value),]
S11<-S10[!grepl("Security Level", S10$s_graph_value),]
S12<-S11[!grepl("Null reading", S11$s_graph_value),]
S13<-S12[!grepl("Cloud cover (%)", S12$s_graph_value),]
S14<-S13[!grepl("Flow status", S13$s_graph_value),]
S15<-S14[!grepl("Tide status", S14$s_graph_value),]
S15<-S14[!grepl("Secchi depth (m)", S14$s_graph_value),]
sensorslist <-S15
usethis::use_data(sensorslist, overwrite = TRUE)
methydro <- read.csv("www/Met_Hydro_sites.csv")
sensorslist_hydromet <- methydro[,1:3]
usethis::use_data(sensorslist_hydromet, overwrite = TRUE)
sensorslist_est <- read.csv("www/sensorslist_est.csv")
usethis::use_data(sensorslist_est, overwrite = TRUE)
sensorslist_oxy <- read.csv("www/sensorslist_oxy.csv")
usethis::use_data(sensorslist_oxy, overwrite = TRUE)
View(S15)
View(S8)
#s1 <- read.csv("C:\\Users\\AminaSaeed\\Desktop\\old_one\\inst\\extdata\\sensorslist.csv") ### pcik DO for the list
#s1 <- read.csv("inst/extdata/sensorInfo.csv")
s1 <- read.csv("www/sensorslist.csv")
#### remove unnecessary columns for catchment
S2<-s1[!grepl("Site Ref", s1$s_graph_value),]
View(S2)
View(s1)
S3<-S2[!grepl("Sample Type", S2$s_graph_value),]
S4<-S3[!grepl("Program Site Ref", S3$s_graph_value),]
S5<-S4[!grepl("Program Code", S4$s_graph_value),]
S6<-S5[!grepl("Collection Method", S5$s_graph_value),]
S7<-S6[!grepl("Depth measuring point", S6$s_graph_value),]
View(S7)
S8<-S7[!grepl("Depth _s (m)", S7$s_graph_value),]
View(S8)
S8<-S7[!grepl("Depth _s(m)", S7$s_graph_value),]
View(S8)
S8<-S7[!grepl("Depth _s (m)", S7$s_graph_value),]
View(S8)
S9<-S8[!grepl("Depth (m)", S8$s_graph_value),]
View(S7)
S8<-S7[!grepl("Depth _s", S7$s_graph_value),]
View(S8)
S9<-S8[!grepl("Depth", S8$s_graph_value),]
View(S9)
S10<-S9[!grepl("Depth To", S9$s_graph_value),]
S10<-S9[!grepl("Depth To", S9$s_graph_value),]
View(S10)
S11<-S10[!grepl("Security Level", S10$s_graph_value),]
S12<-S11[!grepl("Null reading", S11$s_graph_value),]
S13<-S12[!grepl("Cloud cover", S12$s_graph_value),]
S14<-S13[!grepl("Flow status", S13$s_graph_value),]
S15<-S14[!grepl("Tide status", S14$s_graph_value),]
S15<-S14[!grepl("Secchi depth", S14$s_graph_value),]
sensorslist <-S15
usethis::use_data(sensorslist, overwrite = TRUE)
methydro <- read.csv("www/Met_Hydro_sites.csv")
sensorslist_hydromet <- methydro[,1:3]
View(sensorslist_hydromet)
methydro <- read.csv("www/Met_Hydro_sites.csv")
sensorslist_hydromet <- methydro[,1:3]
usethis::use_data(sensorslist_hydromet, overwrite = TRUE)
View(sensorslist_hydromet)
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
gc()
runApp()
shiny::runApp()
#on button click, render stored image
observeEvent(input$wqProfHistFetchData,{
invalidateLater(500, session)
proffolder <- file.path(profdir,input$select_region_profile_hist, input$select_year_profile_hist)
filenames <- list.files(proffolder,pattern=".png",full.names = T)
# proffolder <- paste0(profdir,input$select_region_profile_hist,"/", input$select_year_profile_hist,"/")
# filenames <- awss3Listfiles(prefix = proffolder)
#
#library(slickR)
# observe({
#   # Re-execute this reactive expression after 1000 milliseconds
#   invalidateLater(1000, session)
#
#   # Do something each time this is invalidated.
#   # The isolate() makes this observer _not_ get invalidated and re-executed
#   # when input$n changes.
#   print("test")
# })
output$profPlotHist <- renderSlickR({
#show(
slickR(filenames)+
settings(dots = TRUE,
customPaging = htmlwidgets::JS("function(slick,index) {
return '<a>'+(index+1)+'</a>';
}"))
#)
})
})
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
